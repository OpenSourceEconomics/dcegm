{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: Two-period model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this tutorial is to demonstrate the accuracy of the DCEGM numerical solution method by comparing it with an analytical solution for a simple two-period model. \n",
    "<!-- In this tutorial, we present the *dcegm* package using a simple two-period consumption-retirement model. Its straightforward structure makes it an ideal introduction to this class of models and *dcegm*. -->\n",
    "\n",
    "We begin by presenting the theoretical model. Next, we demonstrate how to solve it numerically using *dcegm*. Finally, we derive its analytical solution confirming the equivalence of the two approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two-period model\n",
    "\n",
    "We consider a simple consumption-retirement model with only two periods. The objective function in period 0 is given by\n",
    "\n",
    "$$ V_0 = \\max_{c_0, d_0} \\mathbb{E}_\\pi \\left[ \\sum_{t=0}^{1} \\beta^{t} u(c_t,d_t)\\right],$$\n",
    "\n",
    "where $\\beta \\in [0,1]$ is the discount factor, $c_t\\geq 0$ is the consumption in period $t$ and $d_t\\in \\{0,1\\}$ specifies the discrete choice with $d_t = 0$ indicating work and $d_t=1$ indicating retirement at the end of period $t$.\n",
    "\n",
    "The utility function has the form\n",
    "\n",
    "$$u(c_t,d_t) = \\frac{c_t^{1-\\rho}}{1-\\rho}-\\delta (1-d_t)+\\epsilon_t(d_t)$$\n",
    "\n",
    "The parameters $\\rho \\geq 0$ and $\\delta \\geq 0$ are measures of risk aversion and  disutility of work, respectively, while $\\epsilon \\sim EV(0,1)$ is a choice-specific taste shock with extreme-value distribution.\n",
    "\n",
    "\n",
    "In each period $t$, the consumption $c_t$ has to satisfy the budget constraint $c_t \\leq M_t$ with wealth\n",
    "\n",
    "$$M_t = R(M_{t-1}-c_{t-1})+W_t(1-d_t)-K D_t,$$\n",
    "\n",
    "where $R$ is the interest factor, $W_t$ is the wage in period $t$ and $D_t$ is an exogenous process indicating long-term care dependence with cost $K$.\n",
    "The wage $W_t = W+\\nu_t$ consists of an average component $W$, which in the two-period model does not depend on $t$ as only the wage in period 1 enters the period 2 budget constraint, and an income shock $\\nu_t \\sim EV(0,1)$.\n",
    "For the exogenous probability of becoming long-term care dependent, we have the following transition probabilities $\\pi$\n",
    "\n",
    "\\begin{align*}\n",
    "\\pi(D_t=1\\mid D_{t-1}=0) & =p_t \\\\\n",
    "\\pi(D_t=1\\mid D_{t-1}=1) &= 1\n",
    "\\end{align*}\n",
    "\n",
    "where $\\pi(D_t=1\\mid D_{t-1}=1)=1$ means that care dependence is absorbing (similar to the retirement decision)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T12:52:20.840911Z",
     "start_time": "2025-04-07T12:52:16.894602Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import dcegm\n",
    "\n",
    "# from dcegm.backward_induction import get_solve_func_for_model\n",
    "# from dcegm.pre_processing.setup_model import create_model_dict\n",
    "# from dcegm.interfaces.interface import policy_and_value_for_state_choice_vec\n",
    "from scipy.special import roots_sh_legendre\n",
    "from scipy.stats import norm\n",
    "\n",
    "from typing import Tuple\n",
    "import jax\n",
    "\n",
    "jax.config.update(\"jax_enable_x64\", True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solve model using DCEGM\n",
    "\n",
    "We now demonstrate how to solve the two-period model outlined above using the *dcegm* package. First, we define the parameters of the model and store them in a dictionary. These parameters are accessed in user-defined functions through the ```params``` argument, which is a required input for all user functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T12:52:20.894017Z",
     "start_time": "2025-04-07T12:52:20.887028Z"
    }
   },
   "outputs": [],
   "source": [
    "params = {}\n",
    "params[\"interest_rate\"] = 0.02\n",
    "params[\"ltc_cost\"] = 5\n",
    "params[\"max_wealth\"] = 50\n",
    "params[\"wage_avg\"] = 8\n",
    "params[\"sigma\"] = 1\n",
    "params[\"taste_shock_scale\"] = 1\n",
    "params[\"ltc_prob\"] = 0.3\n",
    "params[\"beta\"] = 0.95\n",
    "params[\"rho\"] = 0.9\n",
    "params[\"delta\"] = 1.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, the user needs to specify a ```model_config ``` dictionary containing at least the following information:\n",
    "- number of periods,\n",
    "- number of discrete choices,\n",
    "- size of the exogenous wealth grid,\n",
    "- number of stochastic quadrature points\n",
    "\n",
    "Additionally, the user can specify ```\"model_specs\"```, which are passed into all user-supplied functions. These parameters are considered constant and independent of the parametrization of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T12:52:21.071713Z",
     "start_time": "2025-04-07T12:52:21.065261Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Exogenous process: Long-term care dependence (see above)\n",
    "def prob_exog_ltc(ltc, params):\n",
    "    prob_ltc = params[\"ltc_prob\"]\n",
    "\n",
    "    ltc_depenent = ltc == 1\n",
    "    prob_ltc = ltc_depenent + (1 - ltc_depenent) * prob_ltc\n",
    "\n",
    "    return jnp.array([1 - prob_ltc, prob_ltc])\n",
    "\n",
    "\n",
    "model_specs = {\n",
    "    \"choices\": np.arange(2),\n",
    "}\n",
    "\n",
    "model_config = {\n",
    "    \"n_periods\": 2,\n",
    "    \"choices\": np.arange(2),\n",
    "    \"stochastic_states\": {\n",
    "        \"ltc\": [0, 1],\n",
    "    },\n",
    "    \"continuous_states\": {\n",
    "        \"assets_end_of_period\": np.linspace(0, 50, 100),\n",
    "    },\n",
    "    \"n_quad_points\": 5,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will describe the relevant keywords for each user-defined function in the following sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beside the ```params``` and ```options``` dictionaries, the main function of the *dcegm* package ```solve_dcegm``` requires the following inputs:\n",
    "- utility_functions,\n",
    "- budget_constraint,\n",
    "- solve_final_period,\n",
    "- state_space_functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following, we describe the format of the required functions. Note that they must be JAX-compatible, i.e., pure functions that avoid side effects, such as conditional statements. For more details, refer to the [Pure Functions](https://docs.jax.dev/en/latest/notebooks/Common_Gotchas_in_JAX.html) section in the JAX documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utility functions\n",
    "\n",
    "First, we define the utility, the marginal utility and inverse marginal utility. These functions are stored in a dictionary called ```utility_functions```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T12:52:21.123189Z",
     "start_time": "2025-04-07T12:52:21.112584Z"
    }
   },
   "outputs": [],
   "source": [
    "def flow_util(consumption, choice, params):\n",
    "    rho = params[\"rho\"]\n",
    "    delta = params[\"delta\"]\n",
    "    u = consumption ** (1 - rho) / (1 - rho) - delta * (1 - choice)\n",
    "    return u\n",
    "\n",
    "\n",
    "def marginal_utility(consumption, params):\n",
    "    rho = params[\"rho\"]\n",
    "    u_prime = consumption ** (-rho)\n",
    "    return u_prime\n",
    "\n",
    "\n",
    "def inverse_marginal_utility(marginal_utility, params):\n",
    "    rho = params[\"rho\"]\n",
    "    return marginal_utility ** (-1 / rho)\n",
    "\n",
    "\n",
    "utility_functions = {\n",
    "    \"utility\": flow_util,\n",
    "    \"inverse_marginal_utility\": inverse_marginal_utility,\n",
    "    \"marginal_utility\": marginal_utility,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### State space functions\n",
    "\n",
    "The ```state_specific_choice_set``` function specifies the set of feasible choices for each state. It accounts for the fact that retirement is absorbing - that is, if $d_{t-1}=1$, then it must also hold that $d_{t}=1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T12:52:21.205604Z",
     "start_time": "2025-04-07T12:52:21.199109Z"
    }
   },
   "outputs": [],
   "source": [
    "def state_specific_choice_set(\n",
    "    lagged_choice: np.ndarray,\n",
    "    model_specs: np.ndarray,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Select state-specific choice set.\n",
    "\n",
    "    Args:\n",
    "        state (np.ndarray): Array of shape (n_state_variables,) defining the agent's\n",
    "            state. In Ishkakov, an agent's state is defined by her (i) age (i.e., the\n",
    "            current period) and (ii) her lagged labor market choice.\n",
    "            Hence n_state_variables = 2.\n",
    "        state_space (np.ndarray): Collection of all possible states of shape\n",
    "            (n_periods * n_choices, n_choices).\n",
    "        indexer (np.ndarray): Indexer object that maps states to indexes.\n",
    "            Shape (n_periods, n_choices).\n",
    "\n",
    "    Returns:\n",
    "        choice_set (np.ndarray): The agent's (restricted) choice set in the given\n",
    "            state of shape (n_admissible_choices,).\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Once the agent choses retirement, she can only choose retirement thereafter.\n",
    "    # Hence, retirement is an absorbing state.\n",
    "    if lagged_choice == 1:\n",
    "        choice_set = np.array([1])\n",
    "    else:\n",
    "        choice_set = model_specs[\"choices\"]\n",
    "\n",
    "    return choice_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example, consider a where the agent chooses to work in the first period, i.e., $d_0 = 0$. Consequently, in the second period, both choices - $d_1 = 0$ (continue working), $d_1 = 1$ (retire) - are possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T12:52:21.271508Z",
     "start_time": "2025-04-07T12:52:21.259905Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "choice_set_worker = state_specific_choice_set(lagged_choice=0, model_specs=model_specs)\n",
    "choice_set_worker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now consider a case where $d_0 = 1$. In the second period, now only the choice $d_1 = 1$ is possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T12:52:21.346265Z",
     "start_time": "2025-04-07T12:52:21.337424Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "choice_set_retiree = state_specific_choice_set(lagged_choice=1, model_specs=model_specs)\n",
    "choice_set_retiree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ```state_specific_choice_set``` function needs to be placed in a dictionary called ```state_space_functions``` before being passed to the main function ```solve_dcegm```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T12:52:21.480110Z",
     "start_time": "2025-04-07T12:52:21.475239Z"
    }
   },
   "outputs": [],
   "source": [
    "state_space_functions = {\n",
    "    \"state_specific_choice_set\": state_specific_choice_set,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Budget function and transition function\n",
    "\n",
    "Next, we define the budget and transition functions, which take the ```params``` dictionary as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def budget_constraint(\n",
    "    lagged_choice,\n",
    "    ltc,\n",
    "    asset_end_of_previous_period,\n",
    "    income_shock_previous_period,\n",
    "    params,\n",
    "    model_specs,\n",
    "):\n",
    "    interest_factor = 1 + params[\"interest_rate\"]\n",
    "    health_costs = params[\"ltc_cost\"]\n",
    "    wage = params[\"wage_avg\"]\n",
    "    resource = (\n",
    "        interest_factor * asset_end_of_previous_period\n",
    "        + (wage + income_shock_previous_period) * (1 - lagged_choice)\n",
    "        - ltc * health_costs\n",
    "    )\n",
    "    return jnp.maximum(resource, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Already defined above; repeated here for completeness\n",
    "def prob_exog_ltc(ltc, params):\n",
    "    prob_ltc = params[\"ltc_prob\"]\n",
    "\n",
    "    ltc_depenent = ltc == 1\n",
    "    prob_ltc = ltc_depenent + (1 - ltc_depenent) * prob_ltc\n",
    "\n",
    "    return jnp.array([1 - prob_ltc, prob_ltc])\n",
    "\n",
    "\n",
    "stochastic_states_transitions = {\n",
    "    \"ltc\": prob_exog_ltc,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solve final period function\n",
    "\n",
    "Lastly, we need a function that computes the solution in the final period. While this function can be imported directly from the package, we also present it here for clarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T12:52:21.642972Z",
     "start_time": "2025-04-07T12:52:21.637190Z"
    }
   },
   "outputs": [],
   "source": [
    "def final_period_utility(wealth: float, choice: int, params) -> Tuple[float, float]:\n",
    "    return flow_util(wealth, choice, params)\n",
    "\n",
    "\n",
    "def marginal_final(wealth, choice):\n",
    "    return marginal_utility(wealth, params)\n",
    "\n",
    "\n",
    "utility_functions_final_period = {\n",
    "    \"utility\": final_period_utility,\n",
    "    \"marginal_utility\": marginal_final,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solve function\n",
    "\n",
    "Once all input arguments have the required form (see above), they can be passed to the function ```solve_dcegm```. This function returns two multi-dimensional arrays:\n",
    "\n",
    "- policy (np.ndarray): Multi-dimensional np.ndarray storing the choice-specific policy function; of shape [n_states, n_discrete_choices, 2, 1.1 * n_grid_wealth]. Position $[.., 0, :]$ contains the endogenous grid over wealth M, and $[.., 1, :]$ stores the corresponding value of the policy function c(M, d), for each state and each discrete choice.\n",
    "- value (np.ndarray): Multi-dimensional np.ndarray storing the choice-specific value functions; of shape [n_states, n_discrete_choices, 2, 1.1 * n_grid_wealth]. Position $[.., 0, :]$ contains the endogenous grid over wealth M, and $[.., 1, :]$ stores the corresponding value of the value function v(M, d), for each state and each discrete choice.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T12:52:21.810726Z",
     "start_time": "2025-04-07T12:52:21.696902Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update function for state space not given. Assume states only change with an increase of the period and lagged choice.\n",
      "Sparsity condition not provided. Assume all states are valid.\n",
      "Starting state space creation\n",
      "State space created.\n",
      "\n",
      "Starting state-choice space creation and child state mapping.\n",
      "State, state-choice and child state mapping created.\n",
      "\n",
      "Start creating batches for the model.\n",
      "Model setup complete.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = dcegm.setup_model(\n",
    "    model_config=model_config,\n",
    "    model_specs=model_specs,\n",
    "    utility_functions=utility_functions,\n",
    "    utility_functions_final_period=utility_functions_final_period,\n",
    "    state_space_functions=state_space_functions,\n",
    "    stochastic_states_transitions=stochastic_states_transitions,\n",
    "    budget_constraint=budget_constraint,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T12:52:21.840606Z",
     "start_time": "2025-04-07T12:52:21.832391Z"
    }
   },
   "outputs": [],
   "source": [
    "solved_model = model.solve(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_function = solved_model.value\n",
    "policy_function = solved_model.policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analytical solution of the model\n",
    "\n",
    "The solution of the given problem can be derived analytically using backward induction.\n",
    "\n",
    "#### Period 1: \n",
    "The choice problem in period 1 can be expressed through the following Bellman equation\n",
    "$$ V_1 = \\max_{d_1\\in \\{0,1\\}} \\{v_1(M_1,d_1)+\\epsilon_1(d_1)\\}$$\n",
    "Since this is the final period of the model and there is no bequest, agents consume the entire budget, i.e., $c_1 = M_1$. Therefore, the choice-specific value function for a given wealth level $M_1$ and discrete choice $d_1$ is given by\n",
    "$$ v_1(M_1,d_1) = \\frac{M_1^{1-\\rho}}{1-\\rho} - \\delta(1-d_1)$$\n",
    "\n",
    "#### Period 0: \n",
    "Analogous to period 1, the choice problem in period 0 can be expressed through the Bellman equation\n",
    "\n",
    "\n",
    "$$ V_0 = \\max_{d_0\\in \\{0,1\\}} \\{v_0(M_0,d_0)+\\epsilon_0(d_0)\\}$$\n",
    "\n",
    "\n",
    "Here, the choice-specific value function for a given wealth level $M_0$ and choice $d_0$ is defined by\n",
    "\n",
    "\\begin{align*}\n",
    "v_0(M_0,d_0) &= \\max_{d_0\\in\\{0,1\\}} \\{u(c_0,d_0)+\\beta E_0[EV_1(M_1(v_1,D_1))]\\} \\\\\n",
    "&= \\max_{d_0\\in\\{0,1\\}} \\biggl\\{u(c_0,d_0)+\\beta \\sum_{i=1}^{2}\\left(\\int EV_1(M_1(v_1,D_1))\\, \\text{d}f(\\nu)\\right)\\pi(D_1 = i\\mid D_0)\\biggl\\}\n",
    "\\end{align*}\n",
    "\n",
    "where $EV_1(M_1(\\nu_1,D_1))$ is the expected value function for a given realization of the income shock $\\nu_1$ and exogenous process $D_1$, i.e., it is the expected maximum of the different choice-specific value functions in the second period. \n",
    "\n",
    "The extreme value distribution takes the following closed formulas for the expected value function and choice probabilities:\n",
    "\n",
    "$$EV_1(M_1) = \\text{ln}(\\text{exp}(v_1(M_1,1))+\\text{exp}(v_1(M_1,0)))$$\n",
    "\n",
    "$$P(d_1\\mid M_1) = \\frac{\\text{exp}(v_1(M_1,d_1))}{\\text{exp}(v_1(M_1,0))+\\text{exp}(v_1(M_1,1))}$$\n",
    "\n",
    "Now, the problem can be solved using the Euler equation given by (see Iskhakov et al., 2017, Appendix A, Lemma 1)\n",
    "\n",
    "$$u^\\prime(c_0\\mid d_0) = \\beta R E_0\\left[\\sum_{j=1}^{2} u^\\prime(c_1(M_1\\mid d_1),d_1) P(d_1 = j\\mid M_1)\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Policy functions\n",
    "\n",
    "Using the fact that the marginal utility is given by $u^\\prime(c_t) = c_t^{-\\rho}$ we obtain the consumption policy for period 0:\n",
    "\n",
    "\\begin{align*}\n",
    "c_0 &= \\left(\\beta R E_0\\left[\\sum_{j=1}^{2} u^\\prime(c_1(M_1\\mid d_1),d_1) P(d_1 = j\\mid M_1)\\right]\\right)^{-\\frac{1}{\\rho}} \\\\\n",
    "&= \\beta R  \\sum_{i=1}^{2} \\left( \\int \\sum_{j=1}^{2} u^\\prime(c_1(M_1\\mid d_1),d_1) P(d_1 = j\\mid M_1)\\, \\text{d} f(\\nu) \\right)^{-\\frac{1}{\\rho}} \\\\\n",
    "& = \\beta R  \\sum_{i=1}^{2} \\left( \\int \\sum_{j=1}^{2} u^\\prime(M_1,d_1) P(d_1 = j\\mid M_1)\\, \\text{d} f(\\nu) \\right)^{-\\frac{1}{\\rho}}\n",
    "\\end{align*}\n",
    "\n",
    "where we simplified the last equation using the second-period budget constraint $M_1 = c_1$. Note that this policy function is implicit, as $M_1$ depends on $c_0$. More specifically, we have \n",
    "\n",
    "$$M_1 = R(M_0-c_0)+W_2(1-d_0)-K D_1 = R(M_0-c_0)+(W+\\nu_1)(1-d_0)-KD_1.$$\n",
    "\n",
    "The labor supply in period 0 $d_0 \\in \\{0,1\\}$ is the maximizer of $v_0(M_0,d_0)+\\epsilon_0(d_0)$. Hence, $d_0 = 0$ if $v_0(M_0,0)+\\epsilon_0(0)\\geq v_0(M_0,1)+\\epsilon_0(1)$ and $d_0 = 1$ otherwise.\n",
    "\n",
    "Given $c_0$ and $d_0$, the consumption $c_1$ in period 1, which is equal to the wealth $M_1$ can be calculated directly using the budget constraint. The labor supply decision $d_1$ in period 1 can be determined analogously to period 1 as the maximizer of $v_1(M_1,d_1)+\\epsilon_1(d_1)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The budget constraint, wage, transition probability, choice probabilities and the right-hand side of the Euler equation can be implemented as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T12:52:24.860988Z",
     "start_time": "2025-04-07T12:52:24.844262Z"
    }
   },
   "outputs": [],
   "source": [
    "def budget(\n",
    "    lagged_resources, lagged_consumption, lagged_choice, wage, health, params_dict\n",
    "):\n",
    "    interest_factor = 1 + params_dict[\"interest_rate\"]\n",
    "    health_costs = params_dict[\"ltc_cost\"]\n",
    "    resources = (\n",
    "        interest_factor * (lagged_resources - lagged_consumption)\n",
    "        + wage * (1 - lagged_choice)\n",
    "        - health * health_costs\n",
    "    ).clip(min=0.5)\n",
    "    return resources\n",
    "\n",
    "\n",
    "def wage(nu, params_dict):\n",
    "    wage = params_dict[\"wage_avg\"] + nu\n",
    "    return wage\n",
    "\n",
    "\n",
    "def prob_long_term_care_patient(params_dict, lag_health, health):\n",
    "    p = params_dict[\"ltc_prob\"]\n",
    "    if (lag_health == 0) and (health == 1):\n",
    "        pi = p\n",
    "    elif (lag_health == 0) and (health == 0):\n",
    "        pi = 1 - p\n",
    "    elif (lag_health == 1) and (health == 0):\n",
    "        pi = 0\n",
    "    elif (lag_health == 1) and (health == 1):\n",
    "        pi = 1\n",
    "    else:\n",
    "        raise ValueError(\"Health state not defined.\")\n",
    "\n",
    "    return pi\n",
    "\n",
    "\n",
    "def choice_probs(cons, d, params_dict):\n",
    "    v = flow_util(cons, d, params_dict)\n",
    "    v_0 = flow_util(cons, 0, params_dict)\n",
    "    v_1 = flow_util(cons, 1, params_dict)\n",
    "    choice_prob = np.exp(v) / (np.exp(v_0) + np.exp(v_1))\n",
    "    return choice_prob\n",
    "\n",
    "\n",
    "def m_util_aux(init_cond, params_dict, choice_0, nu, consumption):\n",
    "    \"\"\"Return the expected marginal utility for one realization of the wage shock.\"\"\"\n",
    "    budget_0 = init_cond[\"assets_beginning_of_period\"]\n",
    "    health_state_0 = init_cond[\"health\"]\n",
    "\n",
    "    weighted_marginal = 0\n",
    "    for health_state_1 in [0, 1]:\n",
    "        for choice_1 in [0, 1]:\n",
    "            budget_1 = budget(\n",
    "                budget_0,\n",
    "                consumption,\n",
    "                choice_0,\n",
    "                wage(nu, params_dict),\n",
    "                health_state_1,\n",
    "                params_dict,\n",
    "            )\n",
    "            marginal_util = marginal_utility(budget_1, params_dict)\n",
    "            choice_prob = choice_probs(budget_1, choice_1, params_dict)\n",
    "            health_prob = prob_long_term_care_patient(\n",
    "                params_dict, health_state_0, health_state_1\n",
    "            )\n",
    "            weighted_marginal += choice_prob * health_prob * marginal_util\n",
    "\n",
    "    return weighted_marginal\n",
    "\n",
    "\n",
    "def euler_rhs(init_cond, params_dict, draws, weights, choice_0, consumption):\n",
    "    beta = params_dict[\"beta\"]\n",
    "    interest_factor = 1 + params_dict[\"interest_rate\"]\n",
    "\n",
    "    rhs = 0\n",
    "    for index_draw, draw in enumerate(draws):\n",
    "        marg_util_draw = m_util_aux(init_cond, params_dict, choice_0, draw, consumption)\n",
    "        rhs += weights[index_draw] * marg_util_draw\n",
    "    return rhs * beta * interest_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method model_solved.value_and_policy_for_state_and_choice of <dcegm.interfaces.sol_interface.model_solved object at 0x0000019B3EB4BBE0>>"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "solved_model.value_and_policy_for_state_and_choice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of DC-EGM algorithm and analytical solution\n",
    "\n",
    "We now demonstrate the accuracy of the DC-EGM algorithm by substituting the calculated consumption policy into the Euler equation, showing that both sides are approximately equal. \n",
    "\n",
    "For example, consider a specific state in the state space where initial health is $D_0 = 0$ and initial wealth $M_0$ is taken from the first (non-zero) entry on the wealth grid. Additionally, let $d_0 = 0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T12:52:24.893239Z",
     "start_time": "2025-04-07T12:52:24.888467Z"
    }
   },
   "outputs": [],
   "source": [
    "choice_in_period_0 = 0\n",
    "\n",
    "# Prepare dictionary for closed form solution\n",
    "initial_condition = {\"health\": 0, \"assets_beginning_of_period\": 25}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T12:52:25.897984Z",
     "start_time": "2025-04-07T12:52:24.943096Z"
    }
   },
   "outputs": [],
   "source": [
    "state_dict = {\n",
    "    \"ltc\": initial_condition[\"health\"],\n",
    "    \"lagged_choice\": 0,\n",
    "    \"period\": 0,\n",
    "    \"assets_begin_of_period\": initial_condition[\"assets_beginning_of_period\"],\n",
    "    \"choice\": choice_in_period_0,\n",
    "}\n",
    "\n",
    "choice = choice_in_period_0\n",
    "\n",
    "value, cons_calc = solved_model.value_and_policy_for_state_and_choice(\n",
    "    # endog_grid_solved=solved_model.endog_grid,\n",
    "    # value_solved=value_function,\n",
    "    # policy_solved=policy_function,\n",
    "    # params=params,\n",
    "    # model=model,\n",
    "    # state_choice_vec=state_dict,\n",
    "    # wealth=initial_condition[\"wealth\"],\n",
    "    # compute_utility=model.model_funcs[\"compute_utility\"],\n",
    "    state=state_dict,\n",
    "    choice=choice,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the consumption policy calculated by *dcegm*, we now compare the left-hand side of the Euler equation (which is equal to the marginal utility of consumption) to its right-hand side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T12:52:25.939345Z",
     "start_time": "2025-04-07T12:52:25.933917Z"
    }
   },
   "outputs": [],
   "source": [
    "# needed for computation of the integral\n",
    "quad_points, quad_weights = roots_sh_legendre(5)\n",
    "quad_draws = norm.ppf(quad_points) * 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the consumption in the first period $c_0$, as calculated by the DC-EGM algorithm, satisfies the Euler equation because both sides are (approximately) equal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T12:52:26.197491Z",
     "start_time": "2025-04-07T12:52:25.982465Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(0.19613507, dtype=float64)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rhs = euler_rhs(\n",
    "    initial_condition, params, quad_draws, quad_weights, choice_in_period_0, cons_calc\n",
    ")\n",
    "rhs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T12:52:26.241196Z",
     "start_time": "2025-04-07T12:52:26.230872Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(0.05644875, dtype=float64)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lhs = marginal_utility(cons_calc, params)\n",
    "lhs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
